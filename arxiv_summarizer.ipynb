{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6a7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (0.28.0)\n",
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c14022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a6b8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarization Leakage and the IXPE PSF 2401.10231v1\n",
      "ChatQA: Building GPT-4 Level Conversational QA Models 2401.10225v1\n",
      "Supervised Fine-tuning in turn Improves Visual Foundation Models 2401.10222v1\n",
      "Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems 2401.10207v1\n",
      "Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction 2401.10189v1\n",
      "Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields 2401.10178v1\n",
      "DualTake: Predicting Takeovers across Mobilities for Future Personalized Mobility Services 2401.10175v1\n",
      "Explicitly Disentangled Representations in Object-Centric Learning 2401.10148v1\n",
      "Model Compression Techniques in Biometrics Applications: A Survey 2401.10139v1\n",
      "Spatial-Temporal Large Language Model for Traffic Prediction 2401.10134v1\n",
      "['Polarization Leakage and the IXPE PSF', 'ChatQA: Building GPT-4 Level Conversational QA Models', 'Supervised Fine-tuning in turn Improves Visual Foundation Models', 'Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems', 'Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction', 'Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields', 'DualTake: Predicting Takeovers across Mobilities for Future Personalized Mobility Services', 'Explicitly Disentangled Representations in Object-Centric Learning', 'Model Compression Techniques in Biometrics Applications: A Survey', 'Spatial-Temporal Large Language Model for Traffic Prediction']\n",
      "{'2401.10231v1': 'Polarization Leakage and the IXPE PSF', '2401.10225v1': 'ChatQA: Building GPT-4 Level Conversational QA Models', '2401.10222v1': 'Supervised Fine-tuning in turn Improves Visual Foundation Models', '2401.10207v1': 'Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems', '2401.10189v1': 'Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction', '2401.10178v1': 'Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields', '2401.10175v1': 'DualTake: Predicting Takeovers across Mobilities for Future Personalized Mobility Services', '2401.10148v1': 'Explicitly Disentangled Representations in Object-Centric Learning', '2401.10139v1': 'Model Compression Techniques in Biometrics Applications: A Survey', '2401.10134v1': 'Spatial-Temporal Large Language Model for Traffic Prediction'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"artificial intelligence\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "id_dict = {}\n",
    "\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "  paper_id = r.entry_id.split('/')[-1]\n",
    "  id_dict[paper_id] = r.title\n",
    "  print(r.title, paper_id)\n",
    "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "all_results = list(results)\n",
    "print([r.title for r in all_results])\n",
    "\n",
    "print(id_dict)\n",
    "\n",
    "list(id_dict.keys())[0]\n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[list(id_dict.keys())[0]])))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "f_name = '2'\n",
    "paper.download_pdf(filename = f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4009110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import PyPDF2\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17efc44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '2'\n",
    "openai.api_key  = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567dea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "  response = openai.ChatCompletion.create(\n",
    "     model=model,\n",
    "     messages=messages,\n",
    "     temperature=0, # this is the degree of randomness of the model's output\n",
    "  )\n",
    "  return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ed9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pdf file object\n",
    "pdfFileObject = open(filepath, 'rb')\n",
    "# creating a pdf reader object\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObject)\n",
    "text=[]\n",
    "summary=' '\n",
    "#Storing the pages in a list\n",
    "for i in range(0,len(pdfReader.pages)):\n",
    "  # creating a page object\n",
    "  pageObj = pdfReader.pages[i].extract_text()\n",
    "  pageObj= pageObj.replace('\\t\\r','')\n",
    "  pageObj= pageObj.replace('\\xa0','')\n",
    "  # extracting text from page\n",
    "  text.append(pageObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f7b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper discusses the issue of polarization leakage in the gas pixel detectors of the Imaging X-ray Polarimetry Explorer (IXPE) satellite. The detectors measure photoelectron tracks and provide estimates of the photon detection location and its electric vector position angle (EVPA). However, imperfections in reconstructing event positions lead to blurred images and artificial polarized halos around bright sources. \n",
      "\n",
      "The paper introduces a new model to describe polarization leakage and uses it to recover the on-orbit telescope point-spread functions (PSFs), which are useful for faint source detection and image reconstruction. The PSFs obtained from this model are more accurate than previous approximations or ground-calibrated products. \n",
      "\n",
      "The paper also presents an algorithm for polarization leakage correction that is more accurate than existing prescriptions. The corrections depend on the reconstruction method used, and the paper provides prescriptions for both the mission-standard \"Moments\" methods and the \"Neural Net\" event reconstruction. \n",
      "\n",
      "Furthermore, the paper discusses a method to isolate leakage contributions to polarization observations of extended sources and demonstrates that an accurate PSF allows for the extraction of sub-PSF-scale polarization patterns. \n",
      "\n",
      "Overall, the paper addresses the challenges posed by polarization leakage in IXPE data and provides improved methods for PSF recovery and leakage correction.\n",
      "The text provides information about the research paper's methods and findings related to polarization leakage in the context of IXPE (Imaging X-ray Polarimetry Explorer) measurements. Here are the key points:\n",
      "\n",
      "- Figure 1 shows real IXPE level 1 tracks at different energies with reconstructed positions and EVPAs (Mom in purple, NN in red). The counts pile up in the Bragg peak at the end of the photoelectron track. The ellipses represent the uncertainty on the reconstructed position. An example spectral distribution is shown in the background to highlight the rarity of high-energy tracks.\n",
      "\n",
      "- The paper presents a generalized formalism for point source polarization leakage, considering asymmetric PSFs (Point Spread Functions) and the probability distribution of reconstruction errors.\n",
      "\n",
      "- The probability distribution for the detected position of a photon is given by Equation (1), which takes into account the PSF and the probability distribution of reconstruction errors.\n",
      "\n",
      "- Equations (2a), (2b), and (2c) describe how the average contribution of each photoelectron to the I, Q, and U maps of an observation is obtained by averaging over the possible values of the EVPA.\n",
      "\n",
      "- Equation (3) represents the probability distribution of the EVPA, taking into account the source polarization degree, polarization angle, and detector modulation factor.\n",
      "\n",
      "- The paper concludes by noting the limitations of the present treatment and discussing the potential applications of the research for various IXPE measurements.\n",
      "\n",
      "Overall, the paper focuses on addressing polarization leakage in IXPE measurements and proposes a formalism to extract source polarization from observations of extended sources.\n",
      "The text discusses the mathematical equations and parameters used to predict polarization leakage in astronomical observations. The equations involve the expansion of the integrand, the use of a discrete point spread function (PSF) image, and the assumption of uncorrelated reconstruction errors in the parallel and perpendicular directions. The leakage pattern predictions are controlled by three parameters: σ∥ and σ⊥, which control the scale of reconstruction error, and k∥, which models the non-Gaussianity of reconstruction error. These parameters depend on the photon energy. The equations also show the contribution of each photoelectron to the I, Q, and U maps, and how these maps are affected by polarization leakage. If the source is polarized, additional terms are added to the equations.\n",
      "The text discusses the use of photoelectron weight µ and σ∥ to produce maps. It mentions that these terms bias the blurring of I according to the source polarization and add a PSF-distributed source Q and U map to the leakage pattern. The importance of leakage correction is highlighted, especially when source polarizations are low.\n",
      "\n",
      "The text also explains the generalization of leakage patterns to extended sources. It states that the observed leakage pattern for an extended source is a convolution of the point source leakage patterns with the extended source Isrc(r), Qsrc(r), and Usrc(r). It mentions that leakage effects can be less visually prominent in extended source observations due to destructive interference, but they still need to be modeled and subtracted when mapping an extended source's polarization.\n",
      "\n",
      "The text further describes a method to remove the leakage effect and fit for a source polarization. It mentions that the problem of deriving a source polarization from observed maps is an optimization problem with parameters qsrc and usrc. The function to be minimized is defined as F, which includes the difference between predicted and observed values. An additional regulatory term can be added to minimize local fluctuations caused by over-fitting to noise.\n",
      "\n",
      "The text explains the use of gradient descent to iteratively adjust qsrc and usrc until F is stationary. It mentions that the rate κ is a tuneable parameter and the choice of κ is the Barzilai-Borwein method. The initial conditions for qsrc and usrc are set equal to the Stokes coefficients observed in detector 1.\n",
      "\n",
      "The text also mentions the instability of the algorithm when a few pixels are much brighter than the rest and suggests reducing the gradient for these pixels to ensure convergence. It provides a demonstration of the method on a synthetic nebula and mentions its successful application to real observations of the Crab Pulsar Wind Nebula.\n",
      "\n",
      "Lastly, the text briefly mentions the measurement of ground-calibrated PSFs in the Marshall Space Flight Center Stray Light Test Facility.\n",
      "The researchers in this study aimed to extract a new, \"sky-calibrated\" Point Spread Function (PSF) for each detector from select observations of bright, weakly polarized point sources. They used a neural network-based pipeline to process the level 1 GPD images and produce Imaps for the four sources. The amplitude of the leakage blur was set equal to the root mean squared reconstruction error of 106 events simulated by IXPEobssim for an unpolarized point source. The resulting PSFs were compared to symmetric PSFs used in IXPEobssim and ground-calibrated PSFs. The fit method used gradient descent to minimize the χ2 value comparing the blurred PSF predicted by equations to observations. The PSF fit process was performed on either Mom or NN-processed observations, but the reduced reconstruction errors in the NN-processed data led to PSFs that better matched IXPE observations. The regulatory term used in the fit process helped reduce the effect of noise but also artificially blurred the PSFs. Coarse spatial bins were used to reduce Poisson noise, and the azimuthally averaged radial profile from each MMA in the ground PSF data was used to extend the corresponding deconvolved sky PSF core. The faint wings of the sky-calibrated PSFs were dominated by Poisson fluctuations, so the profile was extended to large angles and smoothed to provide PSF tails matched to the core PSF maps. The researchers also fit for leakage model parameters using simulated tracks and measured σ∥, σ⊥, and k∥ in energy bins. The rescaled parameters improved the fit between observed and predicted I, Q, and U maps.\n",
      "The text provides information about leakage prediction and data comparison in the context of a research paper. The leakage parameters as a function of event energy are shown in Figure 4. The leakage prediction for an extended source takes approximately 1 ms per 1,000 pixels on one core. Extracting source polarization for extended sources consumes a few minutes per core. The sky-calibrated PSFs are available on Zenodo and in the supplemental data of the paper. Leakage maps for I, Q, and U are predicted using the sky-calibrated PSFs and compared to observed data. The predicted leakage patterns match well with the observed data, and residual polarization after leakage correction is minimal. The accuracy of leakage prediction is maintained out to the edges of the PSFs, which is valuable for subtracting extended source polarization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text provides information about the comparison of predicted point source images with observed images using different PSFs (Point Spread Functions). The table shows the average event count per detector and the average χ2/pixel errors for the three model PSFs. The sky-calibrated PSFs give significantly improved Q, U models in high count data and improved Imodels at all flux levels. Gaussian blurs with a free scale added to the symmetric and ground-calibrated PSFs only decrease the χ2 for I by about 10%. The sky-calibrated PSFs provide a dramatically improved match to I, even for Mom, with larger improvements for NN. The χ2/pixel for the Q and U maps are much lower than for I in all three PSFs, but the sky-calibrated PSFs still out-perform the other PSFs in leakage prediction, especially for the brightest sources. The asymmetry of the sky-calibrated PSFs is relevant in measuring the polarization of point sources with small apertures. The text also mentions the accuracy of the extended source deconvolution scheme on a synthetic nebula and the use of the leakage model to blur the model I, Q, and U maps into simulated IXPE observations.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     13\u001b[0m summary\u001b[38;5;241m=\u001b[39m summary\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39mresponse \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m19\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(text)):\n",
    "  prompt =f\"\"\"\n",
    "  Your task is to extract relevant information from a text on the page of this research paper. This information will be used to create a summary.\n",
    "  Extract relevant information from the following text, which is delimited with triple backticks.\\\n",
    "  Be sure to preserve the important details.\n",
    "  Text: ```{text[i]}```\n",
    "  \"\"\"\n",
    "  try:\n",
    "    response = get_completion(prompt)\n",
    "  except:\n",
    "    response = get_completion(prompt)\n",
    "  print(response)\n",
    "  summary= summary+' ' +response +'\\n\\n'\n",
    "  time.sleep(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c35768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper focuses on addressing polarization leakage in the gas pixel detectors of the IXPE satellite. It introduces a new model to describe polarization leakage and uses it to recover the on-orbit telescope point-spread functions (PSFs), which are useful for faint source detection and image reconstruction. The paper also presents an algorithm for polarization leakage correction that is more accurate than existing prescriptions. It discusses a method to isolate leakage contributions to polarization observations of extended sources and demonstrates that an accurate PSF allows for the extraction of sub-PSF-scale polarization patterns. The paper provides improved methods for PSF recovery and leakage correction, addressing the challenges posed by polarization leakage in IXPE data.\n"
     ]
    }
   ],
   "source": [
    "prompt =f\"\"\"\n",
    "Your task is to give the most salient points of the research paper in a paragraph form from the provided summary in 100 words.\n",
    "Summary: ```{summary}```\n",
    "\"\"\"\n",
    "summary_2 = get_completion(prompt)\n",
    "print(summary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86ea35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Obtaining dependency information for gtts from https://files.pythonhosted.org/packages/6a/4f/b133719e7638ca68f8805dd75731371db8d5ed23be84d6fc30845a46bedb/gTTS-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading gTTS-2.5.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from gtts) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from gtts) (8.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhamgupta/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts) (2023.7.22)\n",
      "Downloading gTTS-2.5.0-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e4a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4384559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module for text  \n",
    "# to speech conversion \n",
    "from gtts import gTTS \n",
    "  \n",
    "# This module is imported so that we can  \n",
    "# play the converted audio \n",
    "import os \n",
    "  \n",
    "# The text that you want to convert to audio \n",
    "mytext = summary_2\n",
    "  \n",
    "# Language in which you want to convert \n",
    "language = 'en'\n",
    "  \n",
    "# Passing the text and language to the engine,  \n",
    "# here we have marked slow=False. Which tells  \n",
    "# the module that the converted audio should  \n",
    "# have a high speed \n",
    "myobj = gTTS(text=mytext, lang=language, slow=False) \n",
    "  \n",
    "# Saving the converted audio in a mp3 file named \n",
    "# welcome  \n",
    "myobj.save(\"welcome.mp3\") \n",
    "  \n",
    "# # Playing the converted file \n",
    "# os.system(\"mpg321 welcome.mp3\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
